{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Needed Modules","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# Input data files are available in the read-only \"../input/\" directory\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T20:24:42.339196Z","iopub.execute_input":"2021-08-19T20:24:42.339629Z","iopub.status.idle":"2021-08-19T20:24:42.788220Z","shell.execute_reply.started":"2021-08-19T20:24:42.339547Z","shell.execute_reply":"2021-08-19T20:24:42.786525Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n  '\"sox\" backend is being deprecated. '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Audio Data","metadata":{}},{"cell_type":"code","source":"data_path = \"../input/urbansound8k/\"\ntrain_paths = [os.path.join(data_path, f\"fold{i}\") for i in range (1, 10)]\ntest_paths = [os.path.join(data_path, \"fold10\")]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:42.790024Z","iopub.execute_input":"2021-08-19T20:24:42.790393Z","iopub.status.idle":"2021-08-19T20:24:42.795638Z","shell.execute_reply.started":"2021-08-19T20:24:42.790334Z","shell.execute_reply":"2021-08-19T20:24:42.794529Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"info_df = pd.read_csv(os.path.join(data_path, \"UrbanSound8K.csv\"))\ninfo_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:42.797806Z","iopub.execute_input":"2021-08-19T20:24:42.798183Z","iopub.status.idle":"2021-08-19T20:24:42.838811Z","shell.execute_reply.started":"2021-08-19T20:24:42.798146Z","shell.execute_reply":"2021-08-19T20:24:42.837769Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      slice_file_name    fsID  start        end  salience  fold  classID  \\\n0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n\n              class  \n0          dog_bark  \n1  children_playing  \n2  children_playing  \n3  children_playing  \n4  children_playing  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>slice_file_name</th>\n      <th>fsID</th>\n      <th>start</th>\n      <th>end</th>\n      <th>salience</th>\n      <th>fold</th>\n      <th>classID</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100032-3-0-0.wav</td>\n      <td>100032</td>\n      <td>0.0</td>\n      <td>0.317551</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>dog_bark</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100263-2-0-117.wav</td>\n      <td>100263</td>\n      <td>58.5</td>\n      <td>62.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100263-2-0-121.wav</td>\n      <td>100263</td>\n      <td>60.5</td>\n      <td>64.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100263-2-0-126.wav</td>\n      <td>100263</td>\n      <td>63.0</td>\n      <td>67.000000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100263-2-0-137.wav</td>\n      <td>100263</td>\n      <td>68.5</td>\n      <td>72.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class ToMono(torch.nn.Module):\n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        return torch.mean(waveform, dim=0, keepdim=True)\n\nclass Normalize(torch.nn.Module):\n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        return (waveform-waveform.mean()) / waveform.std()\n\nclass Pad(torch.nn.Module):\n    def __init__(self, value: float, size: int):\n        super(Pad, self).__init__()\n        self.value = value\n        self.size = size\n    \n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        return torch.nn.functional.pad(waveform, (0, self.size-max(waveform.shape)), \"constant\", self.value)\n\naudio_transform = torch.nn.Sequential(*[\n    ToMono(), #converts audio channels to mono \n    torchaudio.transforms.Resample(orig_freq=441000, new_freq=8000), # downsamples audio signal to 8000 HZ\n    Normalize(), # normalize audio signal to have mean=0 & std=1\n    Pad(value=0, size=32000),\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:42.840528Z","iopub.execute_input":"2021-08-19T20:24:42.840899Z","iopub.status.idle":"2021-08-19T20:24:42.849905Z","shell.execute_reply.started":"2021-08-19T20:24:42.840851Z","shell.execute_reply":"2021-08-19T20:24:42.848966Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class UrbanSoundDataset(torch.utils.data.Dataset):\n    def __init__(self, paths, info_df, transform=None):\n        self.transform = transform\n        self.info_df = info_df\n        self.file_list = []\n        for path in paths:\n            self.file_list.extend(glob(os.path.join(path, \"*.wav\")))\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        filepath = self.file_list[idx]\n        _, filename = os.path.split(filepath)\n        label = int(self.info_df[self.info_df[\"slice_file_name\"] == filename][\"classID\"])\n        waveform, sample_rate = torchaudio.load(filepath)\n        if self.transform:\n            waveform = self.transform(waveform) \n        return waveform, label","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:42.851467Z","iopub.execute_input":"2021-08-19T20:24:42.852060Z","iopub.status.idle":"2021-08-19T20:24:42.862426Z","shell.execute_reply.started":"2021-08-19T20:24:42.852022Z","shell.execute_reply":"2021-08-19T20:24:42.861449Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = UrbanSoundDataset(train_paths, info_df, audio_transform)\ntest_data = UrbanSoundDataset(test_paths, info_df, audio_transform)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:42.864311Z","iopub.execute_input":"2021-08-19T20:24:42.864628Z","iopub.status.idle":"2021-08-19T20:24:44.414754Z","shell.execute_reply.started":"2021-08-19T20:24:42.864602Z","shell.execute_reply":"2021-08-19T20:24:44.413808Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# train_data[0][1]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.416153Z","iopub.execute_input":"2021-08-19T20:24:44.416664Z","iopub.status.idle":"2021-08-19T20:24:44.421057Z","shell.execute_reply.started":"2021-08-19T20:24:44.416620Z","shell.execute_reply":"2021-08-19T20:24:44.420036Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"random_seed= 42\nshuffle_dataset = True\n\n# Creating data indices for training and validation splits:\ntrain_indices = list(range(len(train_data)))\n\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(train_indices)\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.424492Z","iopub.execute_input":"2021-08-19T20:24:44.424847Z","iopub.status.idle":"2021-08-19T20:24:44.438527Z","shell.execute_reply.started":"2021-08-19T20:24:44.424810Z","shell.execute_reply":"2021-08-19T20:24:44.437796Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.440254Z","iopub.execute_input":"2021-08-19T20:24:44.440672Z","iopub.status.idle":"2021-08-19T20:24:44.452478Z","shell.execute_reply.started":"2021-08-19T20:24:44.440633Z","shell.execute_reply":"2021-08-19T20:24:44.451722Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.453311Z","iopub.execute_input":"2021-08-19T20:24:44.454431Z","iopub.status.idle":"2021-08-19T20:24:44.515121Z","shell.execute_reply.started":"2021-08-19T20:24:44.454399Z","shell.execute_reply":"2021-08-19T20:24:44.514227Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"class CNN(torch.nn.Module):\n    def __init__(self, channels, conv_kernels, conv_strides, conv_padding, pool_padding, num_classes=10):\n        assert len(conv_kernels) == len(channels) == len(conv_strides) == len(conv_padding)\n        super(CNN, self).__init__()\n        # create conv blocks\n        self.conv_blocks = torch.nn.ModuleList()\n        prev_channel = 1\n        for i in range(len(channels)):\n            # add stacked conv layer\n            block = []\n            for j, conv_channel in enumerate(channels[i]):\n                block.append( torch.nn.Conv1d(in_channels = prev_channel, out_channels = conv_channel, kernel_size = conv_kernels[i], stride = conv_strides[i], padding = conv_padding[i]) )\n                prev_channel = conv_channel\n                # add batch norm layer\n                block.append( torch.nn.BatchNorm1d(prev_channel) )\n                # adding ReLU\n                block.append( torch.nn.ReLU() )\n            self.conv_blocks.append( torch.nn.Sequential(*block) )\n\n        # create pool blocks\n        self.pool_blocks = torch.nn.ModuleList()\n        for i in range(len(pool_padding)):\n            # adding Max Pool (drops dims by a factor of 4)\n            self.pool_blocks.append( torch.nn.MaxPool1d(kernel_size = 4, stride = 4, padding = pool_padding[i]) )\n\n        # global pooling\n        self.global_pool = torch.nn.AdaptiveAvgPool1d(1)\n        self.linear = torch.nn.Linear(prev_channel, num_classes)\n\n\n    def forward(self, inwav):\n        for i in range(len(self.conv_blocks)):\n            # apply conv layer\n            inwav = self.conv_blocks[i](inwav)\n            # apply max_pool\n            if i < len(self.pool_blocks): inwav = self.pool_blocks[i](inwav)\n        # apply global pooling\n        out = self.global_pool(inwav).squeeze()\n        out = self.linear(out)\n        return out.squeeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.517071Z","iopub.execute_input":"2021-08-19T20:24:44.517389Z","iopub.status.idle":"2021-08-19T20:24:44.530140Z","shell.execute_reply.started":"2021-08-19T20:24:44.517335Z","shell.execute_reply":"2021-08-19T20:24:44.529131Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ResBlock(torch.nn.Module):\n    def __init__(self, prev_channel, channel, conv_kernel, conv_stride, conv_pad):\n        super(ResBlock, self).__init__()\n        self.res = torch.nn.Sequential(\n            torch.nn.Conv1d(in_channels = prev_channel, out_channels = channel, kernel_size = conv_kernel, stride = conv_stride, padding = conv_pad),\n            torch.nn.BatchNorm1d(channel),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(in_channels = channel, out_channels = channel, kernel_size = conv_kernel, stride = conv_stride, padding = conv_pad),\n            torch.nn.BatchNorm1d(channel),\n        )\n        self.bn = torch.nn.BatchNorm1d(channel)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        x = self.res(x)\n        if x.shape[1] == identity.shape[1]:\n            x += identity\n        # repeat the smaller block till it reaches the size of the bigger block\n        elif x.shape[1] > identity.shape[1]:\n            if x.shape[1] % identity.shape[1] == 0:\n                x += identity.repeat(1, x.shape[1]//identity.shape[1], 1)\n            else:\n                raise RuntimeError(\"Dims in ResBlock needs to be divisible on the previous dims!!\")\n        else:\n            if identity.shape[1] % x.shape[1] == 0:\n                identity += x.repeat(1, identity.shape[1]//x.shape[1], 1)\n            else:\n                raise RuntimeError(\"Dims in ResBlock needs to be divisible on the previous dims!!\")\n            x = identity\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\n\nclass CNNRes(torch.nn.Module):       \n        \n    def __init__(self, channels, conv_kernels, conv_strides, conv_padding, pool_padding, num_classes=10):\n        assert len(conv_kernels) == len(channels) == len(conv_strides) == len(conv_padding)\n        super(CNNRes, self).__init__()\n        \n        # create conv block\n        prev_channel = 1\n        self.conv_block = torch.nn.Sequential(\n            torch.nn.Conv1d(in_channels = prev_channel, out_channels = channels[0][0], kernel_size = conv_kernels[0], stride = conv_strides[0], padding = conv_padding[0]),\n            # add batch norm layer\n            torch.nn.BatchNorm1d(channels[0][0]),\n            # adding ReLU\n            torch.nn.ReLU(),\n            # adding max pool\n            torch.nn.MaxPool1d(kernel_size = 4, stride = 4, padding = pool_padding[0]),\n        )\n        \n        # create res\n        prev_channel = channels[0][0]\n        self.res_blocks = torch.nn.ModuleList()\n        for i in range(1, len(channels)):\n            # add stacked res layer\n            block = []\n            for j, conv_channel in enumerate(channels[i]):\n                block.append( ResBlock(prev_channel, conv_channel, conv_kernels[i], conv_strides[i], conv_padding[i]) )\n                prev_channel = conv_channel\n            self.res_blocks.append( torch.nn.Sequential(*block) )\n\n        # create pool blocks\n        self.pool_blocks = torch.nn.ModuleList()\n        for i in range(1, len(pool_padding)):\n            # adding Max Pool (drops dims by a factor of 4)\n            self.pool_blocks.append( torch.nn.MaxPool1d(kernel_size = 4, stride = 4, padding = pool_padding[i]) )\n\n        # global pooling\n        self.global_pool = torch.nn.AdaptiveAvgPool1d(1)\n        self.linear = torch.nn.Linear(prev_channel, num_classes)\n\n\n    def forward(self, inwav):\n        inwav = self.conv_block(inwav)\n        for i in range(len(self.res_blocks)):\n            # apply conv layer\n            inwav = self.res_blocks[i](inwav)\n            # apply max_pool\n            if i < len(self.pool_blocks): inwav = self.pool_blocks[i](inwav)\n        # apply global pooling\n        out = self.global_pool(inwav).squeeze()\n        out = self.linear(out)\n        return out.squeeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.531803Z","iopub.execute_input":"2021-08-19T20:24:44.532409Z","iopub.status.idle":"2021-08-19T20:24:44.553551Z","shell.execute_reply.started":"2021-08-19T20:24:44.532346Z","shell.execute_reply":"2021-08-19T20:24:44.552719Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"m3 = CNN(channels = [[256], [256]],\n         conv_kernels = [80, 3],\n         conv_strides = [4, 1],\n         conv_padding = [38, 1],\n         pool_padding = [0, 0])\n\nm5 = CNN(channels = [[128], [128], [256], [512]],\n         conv_kernels = [80, 3, 3, 3],\n         conv_strides = [4, 1, 1, 1],\n         conv_padding = [38, 1, 1, 1],\n         pool_padding = [0, 0, 0, 2])\n\nm11 = CNN(channels = [[64], [64]*2, [128]*2, [256]*3, [512]*2],\n          conv_kernels = [80, 3, 3, 3, 3],\n          conv_strides = [4, 1, 1, 1, 1],\n          conv_padding = [38, 1, 1, 1, 1],\n          pool_padding = [0, 0, 0, 2])\n\nm18 = CNN(channels = [[64], [64]*4, [128]*4, [256]*4, [512]*4],\n          conv_kernels = [80, 3, 3, 3, 3],\n          conv_strides = [4, 1, 1, 1, 1],\n          conv_padding = [38, 1, 1, 1, 1],\n          pool_padding = [0, 0, 0, 2])\n\nm32 = CNNRes(channels = [[48], [48]*3, [96]*4, [192]*6, [384]*3],\n          conv_kernels = [80, 3, 3, 3, 3],\n          conv_strides = [4, 1, 1, 1, 1],\n          conv_padding = [38, 1, 1, 1, 1],\n          pool_padding = [0, 0, 0, 2])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.554851Z","iopub.execute_input":"2021-08-19T20:24:44.555497Z","iopub.status.idle":"2021-08-19T20:24:44.694795Z","shell.execute_reply.started":"2021-08-19T20:24:44.555437Z","shell.execute_reply":"2021-08-19T20:24:44.693931Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Train & Test Functions","metadata":{}},{"cell_type":"code","source":"def test(model, data_loader, verbose=False):\n    \"\"\"Measures the accuracy of a model on a data set.\"\"\" \n    # Make sure the model is in evaluation mode.\n    model.eval()\n    correct = 0\n#     print('----- Model Evaluation -----')\n    # We do not need to maintain intermediate activations while testing.\n    with torch.no_grad():\n        # Loop over test data.\n        for features, target in tqdm(data_loader, total=len(data_loader.batch_sampler), desc=\"Testing\"):\n            # Forward pass.\n            output = model(features.to(device))\n            # Get the label corresponding to the highest predicted probability.\n            pred = output.argmax(dim=1, keepdim=True)\n            # Count number of correct predictions.\n            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n    # Print test accuracy.\n    percent = 100. * correct / len(data_loader.sampler)\n    if verbose:\n        print(f'Test accuracy: {correct} / {len(data_loader.sampler)} ({percent:.0f}%)')\n    return percent\n\n\ndef train(model, criterion, data_loader, test_loader, optimizer, num_epochs):\n    \"\"\"Simple training loop for a PyTorch model.\"\"\" \n    \n    # Move model to the device (CPU or GPU).\n    model.to(device)\n    \n    accs = []\n    # Exponential moving average of the loss.\n    ema_loss = None\n\n#     print('----- Training Loop -----')\n    # Loop over epochs.\n    for epoch in range(num_epochs):\n        tick = time.time()\n        model.train()\n        # Loop over data.\n        for batch_idx, (features, target) in tqdm(enumerate(data_loader), total=len(data_loader.batch_sampler), desc=\"training\"):\n            # Forward pass.\n            output = model(features.to(device))\n            loss = criterion(output.to(device), target.to(device))\n            # Backward pass.\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # NOTE: It is important to call .item() on the loss before summing.\n            if ema_loss is None:\n                ema_loss = loss.item()\n            else:\n                ema_loss += (loss.item() - ema_loss) * 0.01\n            tock = time.time()\n        acc = test(model, test_loader, verbose=True)\n        accs.append(acc)\n        # Print out progress the end of epoch.\n        print('Epoch: {} \\tLoss: {:.6f} \\t Time taken: {:.6f} seconds'.format(epoch+1, ema_loss, tock-tick),)\n        torch.save(model.state_dict(), f'model_{epoch}.ckpt')\n        print(\"Model Saved!\")\n        if os.path.isfile(f'model_{epoch-1}.ckpt'):\n            os.remove(f'model_{epoch-1}.ckpt')\n    return accs","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.697824Z","iopub.execute_input":"2021-08-19T20:24:44.698105Z","iopub.status.idle":"2021-08-19T20:24:44.710476Z","shell.execute_reply.started":"2021-08-19T20:24:44.698078Z","shell.execute_reply":"2021-08-19T20:24:44.709452Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"code","source":"# using glorot initialization\ndef init_weights(m):\n    if isinstance(m, torch.nn.Conv1d):\n        torch.nn.init.xavier_uniform_(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.711833Z","iopub.execute_input":"2021-08-19T20:24:44.712487Z","iopub.status.idle":"2021-08-19T20:24:44.723235Z","shell.execute_reply.started":"2021-08-19T20:24:44.712447Z","shell.execute_reply":"2021-08-19T20:24:44.722436Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = m5\nprint(\"Num Parameters:\", sum([p.numel() for p in model.parameters()]))\nmodel.apply(init_weights) ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.724636Z","iopub.execute_input":"2021-08-19T20:24:44.725049Z","iopub.status.idle":"2021-08-19T20:24:44.740022Z","shell.execute_reply.started":"2021-08-19T20:24:44.725009Z","shell.execute_reply":"2021-08-19T20:24:44.739094Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Num Parameters: 559114\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (conv_blocks): ModuleList(\n    (0): Sequential(\n      (0): Conv1d(1, 128, kernel_size=(80,), stride=(4,), padding=(38,))\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (1): Sequential(\n      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (2): Sequential(\n      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (3): Sequential(\n      (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n  )\n  (pool_blocks): ModuleList(\n    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n    (1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n    (2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n    (3): MaxPool1d(kernel_size=4, stride=4, padding=2, dilation=1, ceil_mode=False)\n  )\n  (global_pool): AdaptiveAvgPool1d(output_size=1)\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4) #L2 regularization is added","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.741405Z","iopub.execute_input":"2021-08-19T20:24:44.741741Z","iopub.status.idle":"2021-08-19T20:24:44.746612Z","shell.execute_reply.started":"2021-08-19T20:24:44.741706Z","shell.execute_reply":"2021-08-19T20:24:44.745721Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\naccs = train(model, criterion, train_loader, test_loader, optimizer, num_epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:24:44.748033Z","iopub.execute_input":"2021-08-19T20:24:44.748635Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"training: 100%|██████████| 62/62 [04:13<00:00,  4.08s/it]\nTesting: 100%|██████████| 7/7 [00:25<00:00,  3.71s/it]\ntraining:   0%|          | 0/62 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Test accuracy: 265 / 837 (32%)\nEpoch: 1 \tLoss: 2.078481 \t Time taken: 253.271105 seconds\nModel Saved!\n","output_type":"stream"},{"name":"stderr","text":"training:  60%|█████▉    | 37/62 [01:01<00:39,  1.59s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(accs)\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"# Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}